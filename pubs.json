[
    {
        "code": "2021-TVC-light_calibration",
        "title": "A practical and efficient model for intensity calibration of multi-light image collections",
        "authors": "Ruggero Pintus, Alberto Jaspe-Villanueva, Antonio Zorcolo, Markus Hadwiger, and Enrico Gobbetti",
        "year": "2021",
        "type" : "journal",
        "conference": "CGI 2021",
        "journal": "The Visual Computer",
        "journal-data": "37(9): 2755-2767, September 2021",
        "awards": "Best paper award",
        "abstract": "We present a novel practical and efficient mathematical formulation for light intensity calibration of Multi Light Image Collections (MLICs). Inspired by existing and orthogonal calibration methods, we design a hybrid solution that leverages their strengths while overcoming most of their weaknesses. We combine the rationale of approaches based on fixed analytical models with the interpolation scheme of image domain methods. This allows us to minimize the final residual error in light intensity estimation, without imposing an overly constraining illuminant type. Unlike previous approaches, the proposed calibration strategy proved to be simpler, more efficient and versatile, and extremely adaptable in different setup scenarios. We conduct an extensive analysis and validation of our new light model compared to several state-of-the-art techniques, and we show how the proposed solution provides a more reliable outcomes in terms of accuracy and precision, and a more stable calibration across different light positions/orientations, and with a more general light form factor.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2021-JOCCH-marlie-thumb.jpg",
        "teaser_image": "2021-JOCCH-marlie-teaser.jpg",
        "doi": "10.1007/s00371-021-02172-9",
        "lab_website": "https://vccvisualization.org/research/light-calibration/",
        "youtube_video": "SybNmo98DD4",
        "bibtex_id": ""
    },
    {
        "code": "2021-JOCCH-marlie",
        "title": "Web-based Exploration of Annotated Multi-Layered Relightable Image Models",
        "authors": "Alberto Jaspe-Villanueva, Moonisa Ahsan, Ruggero Pintus, Andrea Giachetti, Fabio Marton, and Enrico Gobbetti",
        "year": "2021",
        "type" : "journal",
        "journal": "Journal of Computing and Cultural Heritage (JOCCH)",
        "journal-data": "14(2): 24:1-24:31, May 2021",
        "abstract": "We introduce a novel approach for exploring image-based shape and material models registered with structured descriptive information fused in multi-scale overlays. We represent the objects of interest as a series of registered layers of image-based shape and material data. These layers are represented at different scales and can come out of a variety of pipelines. These layers can include both Reflectance Transformation Imaging representations, and spatially varying normal and Bidirectional Reflectance Distribution Function fields, possibly as a result of fusing multi-spectral data. An overlay image pyramid associates visual annotations to the various scales. The overlay pyramid of each layer is created at data preparation time by either one of the three subsequent methods: (1) by importing it from other pipelines, (2) by creating it with the simple annotation drawing toolkit available within the viewer, and (3) with external image editing tools. This makes it easier for the user to seamlessly draw annotations over the region of interest. At runtime, clients can access an annotated multi-layered dataset by a standard web server. Users can explore these datasets on a variety of devices; they range from small mobile devices to large-scale displays used in museum installations. On all these aforementioned platforms, JavaScript/WebGL2 clients running in browsers are fully capable of performing layer selection, interactive relighting, enhanced visualization, and annotation display. We address the problem of clutter by embedding interactive lenses. This focus-and-context-aware (multiple-layer) exploration tool supports exploration of more than one representation in a single view. That allows mixing and matching of presentation modes and annotation display. The capabilities of our approach are demonstrated on a variety of cultural heritage use-cases. That involves different kinds of annotated surface and material models.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "weko.jpg",
        "teaser_image": "weko.jpg",
        "doi": "10.1145/3430846",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2021:WEA%27",
        "youtube_video": "iixPu3pNEbg",
        "bibtex_id": "Jaspe:2019:WME"
    },
    {
        "code": "2019-GGF-cluttered_floor_plans",
        "title": "Automatic modeling of cluttered multi-room floor plans from panoramic images",
        "authors": "Giovanni Pintore, Fabio Ganovelli, Alberto Jaspe-Villanueva, and Enrico Gobbetti",
        "year": "2019",
        "type" : "journal",
        "journal": "Computers Graphics Forum",
        "abstract": "We present a novel and light-weight approach to capture and reconstruct structured 3D models of multi-room floor plans. Starting from a small set of registered panoramic images, we automatically generate a 3D layout of the rooms and of all the main objects inside. Such a 3D layout is directly suitable for use in a number of real-world applications, such as guidance, location, routing, or content creation for security and energy management. Our novel pipeline introduces several contributions to indoor reconstruction from purely visual data. In particular, we automatically partition panoramic images in a connectivity graph, according to the visual layout of the rooms, and exploit this graph to support object recovery and rooms boundaries extraction. Moreover, we introduce a plane-sweeping approach to jointly reason about the content of multiple images and solve the problem of object inference in a top-down 2D domain. Finally, we combine these methods in a fully automated pipeline for creating a structured 3D model of a multi-room floor plan and of the location and extent of clutter objects. These contribution make our pipeline able to handle cluttered scenes with complex geometry that are challenging to existing techniques. The effectiveness and performance of our approach is evaluated on both real-world and synthetic models.",
        "projects": ["Indoor modelling"],
        "thumbnail_image": "2019-GGF-cluttered_floor_plans-thumb",
        "teaser_image": "2019-GGF-cluttered_floor_plans-teaser",
        "doi": "10.1111/cgf.13842",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Pintore:2019:AMC%27",
        "bibtex_id": ""
    },    
    {
        "code": "2019-GCH-marlie",
        "title": "Web-based Multi-layered Exploration of Annotated Image-based Shape and Material Models",
        "authors": "Alberto Jaspe-Villanueva, Ruggero Pintus, Andrea Giachetti, and Enrico Gobbetti",
        "year": "2019",
        "type" : "conference",
        "conference": "The 16th Eurographics Workshop on Graphics and Cultural Heritage (GCH)",
        "awards": "Best paper award",
        "abstract": "We introduce a novel versatile approach for letting users explore detailed image-based shape and material models integrated with structured, spatially-associated descriptive information. We represent the objects of interest as a series of registered layers of image-based shape and material information. These layers are represented at multiple scales, and can come out of a variety of pipelines and include both RTI representations and spatially-varying normal and BRDF fields, eventually as a result of fusing multi-spectral data. An overlay image pyramid associates visual annotations to the various scales. The overlay pyramid of each layer can be easily authored at data preparation time using widely available image editing tools. At run-time, an annotated multi-layered dataset is made available to clients by a standard web server. Users can explore these datasets on a variety of devices, from mobile phones to large scale displays in museum installations, using JavaScript/WebGL2 clients capable to perform layer selection, interactive relighting and enhanced visualization, annotation display, and focus-and-context multiple-layer exploration using a lens metaphor. The capabilities of our approach are demonstrated on a variety of cultural heritage use cases involving different kinds of annotated surface and material models.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2019-GCH-marlie-thumb.jpg",
        "teaser_image": "2019-GCH-marlie-teaser.jpg",
        "doi": "10.2312/gch.20191346",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2019:WME%27",
        "youtube_video": "iixPu3pNEbg",
        "bibtex_id": ""
    },
    {
        "code": "2019-GCH-crack_detection",
        "title": "Crack Detection in Single- and Multi-Light Images of Painted Surfaces using Convolutional Neural Networks",
        "authors": "Tinsae Dulecha, Andrea Giachetti, Ruggero Pintus, Irina Ciortan, Alberto Jaspe-Villanueva, and Enrico Gobbetti",
        "year": "2019",
        "type" : "conference",
        "conference": "The 16th Eurographics Workshop on Graphics and Cultural Heritage (GCH)",
        "abstract": "Cracks represent an imminent danger for painted surfaces that needs to be alerted before degenerating into more severe aging effects, such as color loss. Automatic detection of cracks from painted surfaces' images would be therefore extremely useful for art conservators; however, classical image processing solutions are not effective to detect them, distinguish them from other lines or surface characteristics. A possible solution to improve the quality of crack detection exploits Multi-Light Image Collections (MLIC), that are often acquired in the Cultural Heritage domain thanks to the diffusion of the Reflectance Transformation Imaging (RTI) technique, allowing a low cost and rich digitization of artworks' surfaces. In this paper, we propose a pipeline for the detection of crack on egg-tempera paintings from multi-light image acquisitions and that can be used as well on single images. The method is based on single or multi-light edge detection and on a custom Convolutional Neural Network able to classify image patches around edge points as crack or non-crack, trained on RTI data. The pipeline is able to classify regions with cracks with good accuracy when applied on MLIC. Used on single images, it can give still reasonable results. The analysis of the performances for different lighting directions also reveals optimal lighting directions.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2019-GCH-crack_detection-thumb.jpg",
        "teaser_image": "2019-GCH-crack_detection-teaser.jpg",
        "doi": "10.2312/gch.20191347",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Dulecha:2019:CDS%27",
        "bibtex_id": ""
    },
    {
        "code": "2018-GCH-rti_evaluation",
        "title": "Objective and Subjective Evaluation of Virtual Relighting from Reflectance Transformation Imaging Data",
        "authors": "Ruggero Pintus, Tinsae Dulecha, Alberto Jaspe-Villanueva, Andrea Giachetti, Irina Ciortan, and Enrico Gobbetti",
        "year": "2018",
        "type" : "conference",
        "conference": "The 15th Eurographics Workshop on Graphics and Cultural Heritage",
        "abstract": "Reflectance Transformation Imaging (RTI) is widely used to produce relightable models from multi-light image collections. These models are used for a variety of tasks in the Cultural Heritage field. In this work, we carry out an objective and subjective evaluation of RTI data visualization. We start from the acquisition of a series of objects with different geometry and appearance characteristics using a common dome-based configuration. We then transform the acquired data into relightable representations using different approaches: PTM, HSH, and RBF. We then perform an objective error estimation by comparing ground truth images with relighted ones in a leave-one-out framework using PSNR and SSIM error metrics. Moreover, we carry out a subjective investigation through perceptual experiments involving end users with a variety of backgrounds. Objective and subjective tests are shown to behave consistently, and significant differences are found between the various methods. While the proposed analysis has been performed on three common and state-of-the-art RTI visualization methods, our approach is general enough to be extended and applied in the future to new developed multi-light processing pipelines and rendering solutions, to assess their numerical precision and accuracy, and their perceptual visual quality.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2018-GCH-rti_evaluation-thumb.jpg",
        "teaser_image": "2018-GCH-rti_evaluation-teaser.jpg",
        "doi": "0.2312/gch.20181344",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Pintus:2018:OSE%27",
        "bibtex_id": ""
    },
    {
        "code": "2018-MSE-artworks_spotlight",
        "title": "Artworks in the Spotlight: Characterization with a Multispectral Dome",
        "authors": "Irina Ciortan, Tinsae Dulecha, Andrea GIachetti, Ruggero Pintus, Alberto Jaspe-Villanueva, and Enrico Gobbetti",
        "year": "2018",
        "type" : "journal",
        "journal": "IOP Conference Series: Materials Science and Engineering",
        "journal-data": "364(1): 012025, 2018",
        "abstract": "We describe the design and realization of a novel multispectral light dome system and the associated software control and calibration tools used to process the acquired data, in a specialized pipeline geared towards the analysis of shape and appearance properties of cultural heritage items. The current prototype dome, built using easily available electronic and lighting components, can illuminate a target of size 20cm x 20cm from 52 directions uniformly distributed in a hemisphere. From each illumination direction, 3 LED lights cover the visible range of the electromagnetic spectrum, as well as long ultraviolet and near infrared. A dedicated control system implemented on Arduino boards connected to a controlling PC fully manages all lighting and a camera to support automated acquisition. The controlling software also allows real-time adjustment of the LED settings, and provides a live-view of the to-be-captured scene. We approach per-pixel light calibration by placing dedicated targets in the focal plane: four black reflective spheres for back-tracing the position of the LED lamps and a planar full-frame white paper to correct for the non-uniformity of radiance. Once the light calibration is safeguarded, the multispectral acquisition of an artwork can be completed in a matter of minutes, resulting in a spot-wise appearance profile, that stores at pixel level the per-frequency intensity value together with the light direction vector. By performing calibrated acquisition of multispectral Reflectance Transformation Imaging (RTI), with our analysis system it is possible to recover surface normals, to characterize matte and specular behavior of materials, and to explore different surface layers thanks to UV-VIS-IR LED light separation. To demonstrate the system features, we present the outcomes of the on-site capture of metallic artworks at the National Archaeological Museum of Cagliari, Sardinia.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2018-MSE-artworks_spotlight-thumb.jpg",
        "teaser_image": "2018-MSE-artworks_spotlight-teaser.jpg",
        "doi": "10.1088/1757-899X/364/1/012025",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Ciortan:2018:ASC%27",
        "bibtex_id": ""
    },
    {
        "code": "2018-EG-dags_tutorial",
        "title": "Voxel DAGs and Multiresolution Hierarchies: From Large-Scale Scenes to Pre-computed Shadows",
        "authors": "Ulf Assarsson, Markus Billeter, Dan Dolonius, Elmar Eisemann, Alberto Jaspe-Villanueva, Leonardo Scandolo, and Erik Sintorn",
        "year": "2018",
        "type" : "tutorial",
        "conference": "Eurographics 2018 Tutorials",
        "abstract": "In this tutorial, we discuss voxel DAGs and multiresolution hierarchies, which are representations that can encode large volumes of data very efficiently. Despite a significant compression ratio, an advantage of these structures is that their content can be efficiently accessed in real-time. This property enables various applications. We begin the tutorial by introducing the concepts of sparsity and of coherency in voxel structures, and explain how a directed acyclic graph (DAG) can be used to represent voxel geometry in a form that exploits both aspects, while remaining usable in its compressed from for e.g. ray casting. In this context, we also discuss extensions that cover the time domain or consider an advanced encoding strategies exploiting symmetries and entropy. We then move on to voxel attributes, such as colors, and explain how to integrate such information with the voxel DAGs. We will provide implementation details and present methods for efficiently constructing the DAGs and also cover how to efficiently access the data structures with e.g. GPU-based ray tracers. The course will be rounded of with a segment on applications. We highlight a few examples and show their results. Pre-computed shadows are a special application, which will be covered in detail. In this context, we also explain how some of previous ideas contribute to multi-resolution hierarchies, which gives an outlook on the potential generality of the presented solutions.",
        "projects": ["Massive models"],
        "thumbnail_image": "2018-EG-dags_tutorial-thumb.jpg",
        "teaser_image": "2018-EG-dags_tutorial-teaser.jpg",
        "doi": "10.2312/egt.20181028",
        "lab_website": "https://www.crs4.it/vic/eg2018-tutorial-voxels/",
        "bibtex_id": ""
    },
    {
        "code": "2017-JCGT-ssvdags",
        "title": "Symmetry-aware Sparse Voxel DAGs (SSVDAGs) for compression-domain tracing of high-resolution geometric scenes",
        "authors": "Alberto Jaspe-Villanueva, Fabio Marton, and Enrico Gobbetti",
        "year": "2017",
        "type" : "journal",
        "journal": "Journal of Computer Graphics Techniques",
        "journal-data": "6(2): 1-30, 2017",
        "abstract": "Voxelized representations of complex 3D scenes are widely used nowadays to accelerate visibility queries in many GPU rendering techniques. Since GPU memory is limited, it is important that these data structures can be kept within a strict memory budget. Recently, directed acyclic graphs (DAGs) have been successfully introduced to compress sparse voxel octrees (SVOs), but they are limited to sharing identical regions of space. In this paper, we show that a more efficient lossless compression of geometry can be achieved, while keeping the same visibility-query performance, by merging subtrees that are identical through a similarity transform, and by exploiting the skewed distribution of references to shared nodes to store child pointers using a variabile bit-rate encoding. We also describe how, by selecting plane reflections along the main grid directions as symmetry transforms, we can construct highly compressed GPU-friendly structures using a fully out-of-core method. Our results demonstrate that state-of-the-art compression and real-time tracing performance can be achieved on high-resolution voxelized representations of real-world scenes of very different characteristics, including large CAD models, 3D scans, and typical gaming models, leading, for instance, to real-time GPU in-core visualization with shading and shadows of the full Boeing 777 at sub-millimetric precision. This article is based on an earlier work: \textit{SSVDAGs: Symmetry-aware Sparse Voxel DAGs, in Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (c) ACM, 2016. 10.1145/2856400.2856420.} We include here a more thorough exposition, a description of alternative construction and tracing methods, as well as additional results. In order to facilitate understanding, evaluation and extensions, the full source code of the method is provided as accompanying material.",
        "projects": ["Massive models"],
        "thumbnail_image": "2017-JCGT-ssvdags-thumb.jpg",
        "teaser_image": "2017-JCGT-ssvdags-teaser.jpg",
        "publisher_link": "https://jcgt.org/published/0006/02/01/",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2017:SSV%27",
        "youtube_video": "foYPjPKFKWw",
        "bibtex_id": ""
    },
    {
        "code": "2016-VMV-peep",
        "title": "PEEP: Perceptually Enhanced Exploration of Pictures",
        "authors": "Marco Agus, Alberto Jaspe-Villanueva, Giovanni Pintore, and Enrico Gobbetti",
        "year": "2016",
        "type" : "conference",
        "conference": "International Workshop on Vision, Modeling and Visualization (VMV)",
        "abstract": "We present a novel simple technique for rapidly creating and presenting interactive immersive 3D exploration experiences of 2D pictures and images of natural and artificial landscapes. Various application domains, ranging from virtual exploration of works of art to street navigation systems, can benefit from the approach. The method, dubbed PEEP, is motivated by the perceptual characteristics of the human visual system in interpreting perspective cues and detecting relative angles between lines. It applies to the common perspective images with zero or one vanishing points, and does not require the extraction of a precise geometric description of the scene. Taking as input a single image without other information, an automatic analysis technique fits a simple but perceptually consistent parametric 3D representation of the viewed space, which is used to drive an indirect constrained exploration method capable to provide the illusion of 3D exploration with realistic monocular (perspective and motion parallax) and binocular (stereo) depth cues. The effectiveness of the method is demonstrated on a variety of casual pictures and exploration configurations, including mobile devices.",
        "projects": [],
        "thumbnail_image": "2016-VMV-peep-thumb.jpg",
        "teaser_image": "2016-VMV-peep-teaser.jpg",
        "doi": "10.2312/vmv.20161347",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Agus:2016:PPE%27",
        "youtube_video": "S6EUIWyks6E",
        "bibtex_id": ""
    },
    {
        "code": "2016-STAG-crs4_vic",
        "title": "CRS4 Visual Computing",
        "authors": "Enrico Gobbetti, Marco Agus, Fabio Bettio, Alberto Jaspe-Villanueva, Fabio Marton, Ruggero Pintus, Giovanni Pintore, and Antonio Zorcolo",
        "year": "2016",
        "type" : "misc",
        "conference": "STAG 2016 Lab Presentations",
        "abstract": "This lab presentation briefly describes the Visual Computing group of the CRS4 research center. Established in 1996, the group primarily focuses on the study, development, and application of technology for acquisition, storage, processing, distribution, and interactive exploration of complex objects and environments. Research is widely published in major journals and conferences, and many of the developed technologies are used (or have been used) in as diverse real-world applications as internet geoviewing, scientific data analysis, surgical training, and cultural heritage study and valorization.",
        "projects": [""],
        "thumbnail_image": "2016-STAG-crs4_vic-thumb.jpg",
        "teaser_image": "2016-STAG-crs4_vic-teaser.jpg",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Gobbetti:2016:CVC%27",
        "bibtex_id": ""
    },
    {
        "code": "2016-I3D-ssvdags",
        "title": "SSVDAGs: Symmetry-aware Sparse Voxel DAGs",
        "authors": "Alberto Jaspe-Villanueva, Fabio Marton, and Enrico Gobbetti",
        "year": "2016",
        "type" : "conference",
        "conference": "ACM i3D",
        "awards": "Best paper selection",
        "abstract": "Voxelized representations of complex 3D scenes are widely used nowadays to accelerate visibility queries in many GPU rendering techniques. Since GPU memory is limited, it is important that these data structures can be kept within a strict memory budget. Recently, directed acyclic graphs (DAGs) have been successfully introduced to compress sparse voxel octrees (SVOs), but they are limited to sharing identical regions of space. In this paper, we show that a more efficient lossless compression of geometry can be achieved, while keeping the same visibility-query performance, by merging subtrees that are identical through a similarity transform, and by exploiting the skewed distribution of references to shared nodes to store child pointers using a variabile bit-rate encoding. We also describe how, by selecting plane reflections along the main grid directions as symmetry transforms, we can construct highly compressed GPU-friendly structures using a fully out-of-core method. Our results demonstrate that state-of-the-art compression and real-time tracing performance can be achieved on high-resolution voxelized representations of real-world scenes of very different characteristics, including large CAD models, 3D scans, and typical gaming models, leading, for instance, to real-time GPU in-core visualization with shading and shadows of the full Boeing 777 at sub-millimetric precision.",
        "projects": ["Massive models"],
        "thumbnail_image": "2016-I3D-ssvdags",
        "teaser_image": "2016-I3D-ssvdags",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2016:SSS%27",
        "youtube_video": "GmQswlkynP0",
        "bibtex_id": ""
    },
    {
        "code": "2016-book_VCEGDT-pbr_architecture_heritage",
        "title": "Virtual Reality and Point-based Rendering in Architecture and Heritage",
        "authors": "Omar A. Mures, Alberto Jaspe-Villanueva, Emilio J. Padrón, and Juan R. Rabuñal",
        "year": "2016",
        "type" : "book chapter",
        "abstract": "Virtual Reality has been a hot research topic since the appearance of computer graphics, but lately there have been huge advances in the form of high quality and affordable commodity hardware, for example with headsets such as the Oculus Rift. The Rift is an upcoming head-mounted virtual reality display, which will soon be available for the mainstream, along with other similar new VR hardware: Sulon Cortex, CastAR, Altergaze, etc. These new devices also offer new possibilities in the field of Augmented Reality, up to now limited to tablets and smartphones as far as commodity hardware is concerned. In fact, Virtual Reality and Augmented Reality technologies have now achieved the point where it can effectively be applied in in conjunction with the aforementioned workflow will yield great advantages for architects, engineers and heritage professionals alike. This article shows new possibilities of application for Virtual Reality and Augmented Reality with massive point clouds in real world architectural and heritage workflows.",
        "projects": ["Massive models", "Point clouds", "Cultral Heritage"],
        "thumbnail_image": "2016-book_VCEGDT-pbr_architecture_heritage",
        "teaser_image": "2016-book_VCEGDT-pbr_architecture_heritage",
        "doi": "10.4018/978-1-5225-0029-2",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mures:2016:VRP%27",
        "youtube_video": "3Zo8v1uk2yA",
        "bibtex_id": ""
    },
    {
        "code": "2016-book_EBDMOI-pcm",
        "title": "Point Cloud Manager: Applications of a Middleware for Managing Huge Point Clouds",
        "authors": "Omar A. Mures, Alberto Jaspe-Villanueva, Emilio J. Padrón, and Juan R. Rabuñal",
        "year": "2016",
        "type" : "book chapter",
        "book": "Effective Big Data Management and Opportunities for Implementation",
        "book-data": "Chapter 13, IGI Global, June 2016",
        "abstract": "Recent advances in acquisition technologies, such as LIDAR and photogrammetry, have brought back to popularity 3D point clouds in a lot of fields of application of Computer Graphics: Civil Engineering, Architecture, Topography, etc. These acquisition systems are producing an unprecedented amount of geometric data with additional attached information, resulting in huge datasets whose processing and storage requirements exceed usual approaches, presenting new challenges that can be addressed from a Big Data perspective by applying High Performance Computing and Computer Graphics techniques. This chapter presents a series of applications built on top of Point Cloud Manager (PCM), a middleware that provides an abstraction for point clouds with arbitrary attached data and makes it easy to perform out-of-core operations on them on commodity CPUs and GPUs. Hence, different kinds of real world applications are tackled, showing both real-time and offline examples, and render-oriented and computation-related operations as well.",
        "projects": ["Massive models", "Point clouds"],
        "thumbnail_image": "2016-book_EBDMOI-pcm-thumb.jpg",
        "teaser_image": "2016-book_EBDMOI-pcm-teaser.jpg",
        "doi": "10.4018/978-1-5225-0182-4.ch013",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mures:2016:PCM%27",
        "youtube_video": "3Zo8v1uk2yA",
        "bibtex_id": ""
    },
    {
        "code": "2015-JOCCH-montescan",
        "title": "Mont'e Scan: Effective Shape and Color Digitization of Cluttered 3D Artworks",
        "authors": "Fabio Bettio, Alberto Jaspe-Villanueva, Emilio Merella, Fabio Marton, Enrico Gobbetti, and Ruggero Pintus",
        "year": "2015",
        "type" : "journal",
        "journal": "ACM Journal on Computing and Cultural Heritage",
        "abstract": "We propose an approach for improving the digitization of shape and color of 3D artworks in a cluttered environment using 3D laser scanning and flash photography. In order to separate clutter from acquired material, semi-automated methods are employed to generate masks used to segment the range maps and the color photographs. This approach allows the removal of unwanted 3D and color data prior to the integration of acquired data in a 3D model. Sharp shadows generated by flash acquisition are easily handled by this masking process, and color deviations introduced by the flash light are corrected at the color blending step by taking into account the geometry of the object. The approach has been evaluated on a large scale acquisition campaign of the Mont'e Prama complex. This site contains an extraordinary collection of stone fragments from the Nuragic era, which depict small models of prehistoric nuraghe (cone-shaped stone towers), as well as larger-than-life archers, warriors, and boxers. The acquisition campaign has covered 37 statues mounted on metallic supports. Color and shape were acquired at a resolution of 0.25mm, which resulted in over 6200 range maps (about 1.3G valid samples) and 3817 photographs.",
        "projects": ["Massive models", "Point clouds", "Cultral Heritage"],
        "thumbnail_image": "2015-JOCCH-montescan",
        "teaser_image": "2015-JOCCH-montescan",
        "doi": "10.1145/2644823",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Bettio:2014:MES%27",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2015-EG-chc+rt",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2015-EG-chc+rt",
        "teaser_image": "2015-EG-chc+rt",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2014-VMV-soar",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2014-VMV-soar",
        "teaser_image": "2014-VMV-soar",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2014-STAG-line_rasterization",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2014-STAG-line_rasterization",
        "teaser_image": "2014-STAG-line_rasterization",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2014-JOCCH-isocam",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2014-JOCCH-isocam",
        "teaser_image": "2014-JOCCH-isocam",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2014-EG-poster_indoor_reconstruction",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2014-EG-poster_indoor_reconstruction",
        "teaser_image": "2014-EG-poster_indoor_reconstruction",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2014-EG-exploremaps",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2014-EG-exploremaps",
        "teaser_image": "2014-EG-exploremaps",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2014-CAG-indoor_architectural_reconstruction",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2014-CAG-indoor_architectural_reconstruction",
        "teaser_image": "2014-CAG-indoor_architectural_reconstruction",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2013-EVIS-ligtfield_display_calibration",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2013-EVIS-ligtfield_display_calibration",
        "teaser_image": "2013-EVIS-ligtfield_display_calibration",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2013-CAD-robust_indoor_reconstruction",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2013-CAD-robust_indoor_reconstruction",
        "teaser_image": "2013-CAD-robust_indoor_reconstruction",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2012-VAR-inspeccion_catedral",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2012-VAR-inspeccion_catedral",
        "teaser_image": "2012-VAR-inspeccion_catedral",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2011-EGA-space_perception",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2011-EGA-space_perception",
        "teaser_image": "2011-EGA-space_perception",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2009-book_EAI-acceleration_ia_gpus",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2009-book_EAI-acceleration_ia_gpus",
        "teaser_image": "2009-book_EAI-acceleration_ia_gpus",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-SIGRADI-cuencas_visuales",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-SIGRADI-cuencas_visuales",
        "teaser_image": "2008-SIGRADI-cuencas_visuales",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-LaSalle-interfaces_naturales_museos",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-LaSalle-interfaces_naturales_museos",
        "teaser_image": "2008-LaSalle-interfaces_naturales_museos",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-ISICTCH-virtual_reality_museum",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-ISICTCH-virtual_reality_museum",
        "teaser_image": "2008-ISICTCH-virtual_reality_museum",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-book_GPU-Gems-mapping_large_textures",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-book_GPU-Gems-mapping_large_textures",
        "teaser_image": "2008-book_GPU-Gems-mapping_large_textures",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2007-WSCG-hardware_independet-clipmapping",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2007-WSCG-hardware_independet-clipmapping",
        "teaser_image": "2007-WSCG-hardware_independet-clipmapping",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2007-SIGPHI-museo_vacio_patrimonio",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2007-SIGPHI-museo_vacio_patrimonio",
        "teaser_image": "2007-SIGPHI-museo_vacio_patrimonio",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2007-IJAC-walking_digital_spaces",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2007-IJAC-walking_digital_spaces",
        "teaser_image": "2007-IJAC-walking_digital_spaces",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2006-SIGRADI-museo_vacio",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2006-SIGRADI-museo_vacio",
        "teaser_image": "2006-SIGRADI-museo_vacio",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2005-ICC-gid_integration",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2005-ICC-gid_integration",
        "teaser_image": "2005-ICC-gid_integration",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2005-cisit-gestion_trafico",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2005-cisit-gestion_trafico",
        "teaser_image": "2005-cisit-gestion_trafico",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2005-CAD-creativity_space",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2005-CAD-creativity_space",
        "teaser_image": "2005-CAD-creativity_space",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    }
]