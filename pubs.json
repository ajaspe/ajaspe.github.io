[
    {
        "code": "2021-TVC-light_calibration",
        "title": "A practical and efficient model for intensity calibration of multi-light image collections",
        "authors": "Ruggero Pintus, Alberto Jaspe-Villanueva, Antonio Zorcolo, Markus Hadwiger, and Enrico Gobbetti",
        "year": "2021",
        "type" : "journal",
        "conference": "CGI 2021",
        "journal": "The Visual Computer",
        "journal-data": "37(9): 2755-2767, September 2021",
        "awards": "Best paper award",
        "abstract": "We present a novel practical and efficient mathematical formulation for light intensity calibration of Multi Light Image Collections (MLICs). Inspired by existing and orthogonal calibration methods, we design a hybrid solution that leverages their strengths while overcoming most of their weaknesses. We combine the rationale of approaches based on fixed analytical models with the interpolation scheme of image domain methods. This allows us to minimize the final residual error in light intensity estimation, without imposing an overly constraining illuminant type. Unlike previous approaches, the proposed calibration strategy proved to be simpler, more efficient and versatile, and extremely adaptable in different setup scenarios. We conduct an extensive analysis and validation of our new light model compared to several state-of-the-art techniques, and we show how the proposed solution provides a more reliable outcomes in terms of accuracy and precision, and a more stable calibration across different light positions/orientations, and with a more general light form factor.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2021-JOCCH-marlie-thumb.jpg",
        "teaser_image": "2021-JOCCH-marlie-teaser.jpg",
        "doi": "10.1007/s00371-021-02172-9",
        "lab_website": "https://vccvisualization.org/research/light-calibration/",
        "youtube_video": "SybNmo98DD4",
        "bibtex_id": ""
    },
    {
        "code": "2021-JOCCH-marlie",
        "title": "Web-based Exploration of Annotated Multi-Layered Relightable Image Models",
        "authors": "Alberto Jaspe-Villanueva, Moonisa Ahsan, Ruggero Pintus, Andrea Giachetti, Fabio Marton, and Enrico Gobbetti",
        "year": "2021",
        "type" : "journal",
        "journal": "Journal of Computing and Cultural Heritage (JOCCH)",
        "journal-data": "14(2): 24:1-24:31, May 2021",
        "abstract": "We introduce a novel approach for exploring image-based shape and material models registered with structured descriptive information fused in multi-scale overlays. We represent the objects of interest as a series of registered layers of image-based shape and material data. These layers are represented at different scales and can come out of a variety of pipelines. These layers can include both Reflectance Transformation Imaging representations, and spatially varying normal and Bidirectional Reflectance Distribution Function fields, possibly as a result of fusing multi-spectral data. An overlay image pyramid associates visual annotations to the various scales. The overlay pyramid of each layer is created at data preparation time by either one of the three subsequent methods: (1) by importing it from other pipelines, (2) by creating it with the simple annotation drawing toolkit available within the viewer, and (3) with external image editing tools. This makes it easier for the user to seamlessly draw annotations over the region of interest. At runtime, clients can access an annotated multi-layered dataset by a standard web server. Users can explore these datasets on a variety of devices; they range from small mobile devices to large-scale displays used in museum installations. On all these aforementioned platforms, JavaScript/WebGL2 clients running in browsers are fully capable of performing layer selection, interactive relighting, enhanced visualization, and annotation display. We address the problem of clutter by embedding interactive lenses. This focus-and-context-aware (multiple-layer) exploration tool supports exploration of more than one representation in a single view. That allows mixing and matching of presentation modes and annotation display. The capabilities of our approach are demonstrated on a variety of cultural heritage use-cases. That involves different kinds of annotated surface and material models.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "weko.jpg",
        "teaser_image": "weko.jpg",
        "doi": "10.1145/3430846",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2021:WEA%27",
        "youtube_video": "iixPu3pNEbg",
        "bibtex_id": "Jaspe:2019:WME"
    },
    {
        "code": "2019-GGF-cluttered_floor_plans",
        "title": "Automatic modeling of cluttered multi-room floor plans from panoramic images",
        "authors": "Giovanni Pintore, Fabio Ganovelli, Alberto Jaspe-Villanueva, and Enrico Gobbetti",
        "year": "2019",
        "type" : "journal",
        "journal": "Computers Graphics Forum",
        "abstract": "We present a novel and light-weight approach to capture and reconstruct structured 3D models of multi-room floor plans. Starting from a small set of registered panoramic images, we automatically generate a 3D layout of the rooms and of all the main objects inside. Such a 3D layout is directly suitable for use in a number of real-world applications, such as guidance, location, routing, or content creation for security and energy management. Our novel pipeline introduces several contributions to indoor reconstruction from purely visual data. In particular, we automatically partition panoramic images in a connectivity graph, according to the visual layout of the rooms, and exploit this graph to support object recovery and rooms boundaries extraction. Moreover, we introduce a plane-sweeping approach to jointly reason about the content of multiple images and solve the problem of object inference in a top-down 2D domain. Finally, we combine these methods in a fully automated pipeline for creating a structured 3D model of a multi-room floor plan and of the location and extent of clutter objects. These contribution make our pipeline able to handle cluttered scenes with complex geometry that are challenging to existing techniques. The effectiveness and performance of our approach is evaluated on both real-world and synthetic models.",
        "projects": ["Indoor modelling"],
        "thumbnail_image": "2019-GGF-cluttered_floor_plans-thumb",
        "teaser_image": "2019-GGF-cluttered_floor_plans-teaser",
        "doi": "10.1111/cgf.13842",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Pintore:2019:AMC%27",
        "bibtex_id": ""
    },    
    {
        "code": "2019-GCH-marlie",
        "title": "Web-based Multi-layered Exploration of Annotated Image-based Shape and Material Models",
        "authors": "Alberto Jaspe-Villanueva, Ruggero Pintus, Andrea Giachetti, and Enrico Gobbetti",
        "year": "2019",
        "type" : "conference",
        "conference": "The 16th Eurographics Workshop on Graphics and Cultural Heritage (GCH)",
        "awards": "Best paper award",
        "abstract": "We introduce a novel versatile approach for letting users explore detailed image-based shape and material models integrated with structured, spatially-associated descriptive information. We represent the objects of interest as a series of registered layers of image-based shape and material information. These layers are represented at multiple scales, and can come out of a variety of pipelines and include both RTI representations and spatially-varying normal and BRDF fields, eventually as a result of fusing multi-spectral data. An overlay image pyramid associates visual annotations to the various scales. The overlay pyramid of each layer can be easily authored at data preparation time using widely available image editing tools. At run-time, an annotated multi-layered dataset is made available to clients by a standard web server. Users can explore these datasets on a variety of devices, from mobile phones to large scale displays in museum installations, using JavaScript/WebGL2 clients capable to perform layer selection, interactive relighting and enhanced visualization, annotation display, and focus-and-context multiple-layer exploration using a lens metaphor. The capabilities of our approach are demonstrated on a variety of cultural heritage use cases involving different kinds of annotated surface and material models.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2019-GCH-marlie-thumb.jpg",
        "teaser_image": "2019-GCH-marlie-teaser.jpg",
        "doi": "10.2312/gch.20191346",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2019:WME%27",
        "youtube_video": "iixPu3pNEbg",
        "bibtex_id": ""
    },
    {
        "code": "2019-GCH-crack_detection",
        "title": "Crack Detection in Single- and Multi-Light Images of Painted Surfaces using Convolutional Neural Networks",
        "authors": "Tinsae Dulecha, Andrea Giachetti, Ruggero Pintus, Irina Ciortan, Alberto Jaspe-Villanueva, and Enrico Gobbetti",
        "year": "2019",
        "type" : "conference",
        "conference": "The 16th Eurographics Workshop on Graphics and Cultural Heritage (GCH)",
        "abstract": "Cracks represent an imminent danger for painted surfaces that needs to be alerted before degenerating into more severe aging effects, such as color loss. Automatic detection of cracks from painted surfaces' images would be therefore extremely useful for art conservators; however, classical image processing solutions are not effective to detect them, distinguish them from other lines or surface characteristics. A possible solution to improve the quality of crack detection exploits Multi-Light Image Collections (MLIC), that are often acquired in the Cultural Heritage domain thanks to the diffusion of the Reflectance Transformation Imaging (RTI) technique, allowing a low cost and rich digitization of artworks' surfaces. In this paper, we propose a pipeline for the detection of crack on egg-tempera paintings from multi-light image acquisitions and that can be used as well on single images. The method is based on single or multi-light edge detection and on a custom Convolutional Neural Network able to classify image patches around edge points as crack or non-crack, trained on RTI data. The pipeline is able to classify regions with cracks with good accuracy when applied on MLIC. Used on single images, it can give still reasonable results. The analysis of the performances for different lighting directions also reveals optimal lighting directions.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2019-GCH-crack_detection-thumb.jpg",
        "teaser_image": "2019-GCH-crack_detection-teaser.jpg",
        "doi": "10.2312/gch.20191347",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Dulecha:2019:CDS%27",
        "bibtex_id": ""
    },
    {
        "code": "2018-GCH-rti_evaluation",
        "title": "Objective and Subjective Evaluation of Virtual Relighting from Reflectance Transformation Imaging Data",
        "authors": "Ruggero Pintus, Tinsae Dulecha, Alberto Jaspe-Villanueva, Andrea Giachetti, Irina Ciortan, and Enrico Gobbetti",
        "year": "2018",
        "type" : "conference",
        "conference": "The 15th Eurographics Workshop on Graphics and Cultural Heritage",
        "abstract": "Reflectance Transformation Imaging (RTI) is widely used to produce relightable models from multi-light image collections. These models are used for a variety of tasks in the Cultural Heritage field. In this work, we carry out an objective and subjective evaluation of RTI data visualization. We start from the acquisition of a series of objects with different geometry and appearance characteristics using a common dome-based configuration. We then transform the acquired data into relightable representations using different approaches: PTM, HSH, and RBF. We then perform an objective error estimation by comparing ground truth images with relighted ones in a leave-one-out framework using PSNR and SSIM error metrics. Moreover, we carry out a subjective investigation through perceptual experiments involving end users with a variety of backgrounds. Objective and subjective tests are shown to behave consistently, and significant differences are found between the various methods. While the proposed analysis has been performed on three common and state-of-the-art RTI visualization methods, our approach is general enough to be extended and applied in the future to new developed multi-light processing pipelines and rendering solutions, to assess their numerical precision and accuracy, and their perceptual visual quality.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2018-GCH-rti_evaluation-thumb.jpg",
        "teaser_image": "2018-GCH-rti_evaluation-teaser.jpg",
        "doi": "0.2312/gch.20181344",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Pintus:2018:OSE%27",
        "bibtex_id": ""
    },
    {
        "code": "2018-MSE-artworks_spotlight",
        "title": "Artworks in the Spotlight: Characterization with a Multispectral Dome",
        "authors": "Irina Ciortan, Tinsae Dulecha, Andrea GIachetti, Ruggero Pintus, Alberto Jaspe-Villanueva, and Enrico Gobbetti",
        "year": "2018",
        "type" : "journal",
        "journal": "IOP Conference Series: Materials Science and Engineering",
        "journal-data": "364(1): 012025, 2018",
        "abstract": "We describe the design and realization of a novel multispectral light dome system and the associated software control and calibration tools used to process the acquired data, in a specialized pipeline geared towards the analysis of shape and appearance properties of cultural heritage items. The current prototype dome, built using easily available electronic and lighting components, can illuminate a target of size 20cm x 20cm from 52 directions uniformly distributed in a hemisphere. From each illumination direction, 3 LED lights cover the visible range of the electromagnetic spectrum, as well as long ultraviolet and near infrared. A dedicated control system implemented on Arduino boards connected to a controlling PC fully manages all lighting and a camera to support automated acquisition. The controlling software also allows real-time adjustment of the LED settings, and provides a live-view of the to-be-captured scene. We approach per-pixel light calibration by placing dedicated targets in the focal plane: four black reflective spheres for back-tracing the position of the LED lamps and a planar full-frame white paper to correct for the non-uniformity of radiance. Once the light calibration is safeguarded, the multispectral acquisition of an artwork can be completed in a matter of minutes, resulting in a spot-wise appearance profile, that stores at pixel level the per-frequency intensity value together with the light direction vector. By performing calibrated acquisition of multispectral Reflectance Transformation Imaging (RTI), with our analysis system it is possible to recover surface normals, to characterize matte and specular behavior of materials, and to explore different surface layers thanks to UV-VIS-IR LED light separation. To demonstrate the system features, we present the outcomes of the on-site capture of metallic artworks at the National Archaeological Museum of Cagliari, Sardinia.",
        "projects": ["RTI", "Cultral Heritage"],
        "thumbnail_image": "2018-MSE-artworks_spotlight-thumb.jpg",
        "teaser_image": "2018-MSE-artworks_spotlight-teaser.jpg",
        "doi": "10.1088/1757-899X/364/1/012025",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Ciortan:2018:ASC%27",
        "bibtex_id": ""
    },
    {
        "code": "2018-EG-dags_tutorial",
        "title": "Voxel DAGs and Multiresolution Hierarchies: From Large-Scale Scenes to Pre-computed Shadows",
        "authors": "Ulf Assarsson, Markus Billeter, Dan Dolonius, Elmar Eisemann, Alberto Jaspe-Villanueva, Leonardo Scandolo, and Erik Sintorn",
        "year": "2018",
        "type" : "tutorial",
        "conference": "Eurographics 2018 Tutorials",
        "abstract": "In this tutorial, we discuss voxel DAGs and multiresolution hierarchies, which are representations that can encode large volumes of data very efficiently. Despite a significant compression ratio, an advantage of these structures is that their content can be efficiently accessed in real-time. This property enables various applications. We begin the tutorial by introducing the concepts of sparsity and of coherency in voxel structures, and explain how a directed acyclic graph (DAG) can be used to represent voxel geometry in a form that exploits both aspects, while remaining usable in its compressed from for e.g. ray casting. In this context, we also discuss extensions that cover the time domain or consider an advanced encoding strategies exploiting symmetries and entropy. We then move on to voxel attributes, such as colors, and explain how to integrate such information with the voxel DAGs. We will provide implementation details and present methods for efficiently constructing the DAGs and also cover how to efficiently access the data structures with e.g. GPU-based ray tracers. The course will be rounded of with a segment on applications. We highlight a few examples and show their results. Pre-computed shadows are a special application, which will be covered in detail. In this context, we also explain how some of previous ideas contribute to multi-resolution hierarchies, which gives an outlook on the potential generality of the presented solutions.",
        "projects": ["Massive models"],
        "thumbnail_image": "2018-EG-dags_tutorial-thumb.jpg",
        "teaser_image": "2018-EG-dags_tutorial-teaser.jpg",
        "doi": "10.2312/egt.20181028",
        "lab_website": "https://www.crs4.it/vic/eg2018-tutorial-voxels/",
        "bibtex_id": ""
    },
    {
        "code": "2017-JCGT-ssvdags",
        "title": "Symmetry-aware Sparse Voxel DAGs (SSVDAGs) for compression-domain tracing of high-resolution geometric scenes",
        "authors": "Alberto Jaspe-Villanueva, Fabio Marton, and Enrico Gobbetti",
        "year": "2017",
        "type" : "journal",
        "journal": "Journal of Computer Graphics Techniques",
        "journal-data": "6(2): 1-30, 2017",
        "abstract": "Voxelized representations of complex 3D scenes are widely used nowadays to accelerate visibility queries in many GPU rendering techniques. Since GPU memory is limited, it is important that these data structures can be kept within a strict memory budget. Recently, directed acyclic graphs (DAGs) have been successfully introduced to compress sparse voxel octrees (SVOs), but they are limited to sharing identical regions of space. In this paper, we show that a more efficient lossless compression of geometry can be achieved, while keeping the same visibility-query performance, by merging subtrees that are identical through a similarity transform, and by exploiting the skewed distribution of references to shared nodes to store child pointers using a variabile bit-rate encoding. We also describe how, by selecting plane reflections along the main grid directions as symmetry transforms, we can construct highly compressed GPU-friendly structures using a fully out-of-core method. Our results demonstrate that state-of-the-art compression and real-time tracing performance can be achieved on high-resolution voxelized representations of real-world scenes of very different characteristics, including large CAD models, 3D scans, and typical gaming models, leading, for instance, to real-time GPU in-core visualization with shading and shadows of the full Boeing 777 at sub-millimetric precision. This article is based on an earlier work: \textit{SSVDAGs: Symmetry-aware Sparse Voxel DAGs, in Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (c) ACM, 2016. 10.1145/2856400.2856420.} We include here a more thorough exposition, a description of alternative construction and tracing methods, as well as additional results. In order to facilitate understanding, evaluation and extensions, the full source code of the method is provided as accompanying material.",
        "projects": ["Massive models"],
        "thumbnail_image": "2017-JCGT-ssvdags-thumb.jpg",
        "teaser_image": "2017-JCGT-ssvdags-teaser.jpg",
        "publisher_link": "https://jcgt.org/published/0006/02/01/",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2017:SSV%27",
        "youtube_video": "foYPjPKFKWw",
        "bibtex_id": ""
    },
    {
        "code": "2016-VMV-peep",
        "title": "PEEP: Perceptually Enhanced Exploration of Pictures",
        "authors": "Marco Agus, Alberto Jaspe-Villanueva, Giovanni Pintore, and Enrico Gobbetti",
        "year": "2016",
        "type" : "conference",
        "conference": "International Workshop on Vision, Modeling and Visualization (VMV)",
        "abstract": "We present a novel simple technique for rapidly creating and presenting interactive immersive 3D exploration experiences of 2D pictures and images of natural and artificial landscapes. Various application domains, ranging from virtual exploration of works of art to street navigation systems, can benefit from the approach. The method, dubbed PEEP, is motivated by the perceptual characteristics of the human visual system in interpreting perspective cues and detecting relative angles between lines. It applies to the common perspective images with zero or one vanishing points, and does not require the extraction of a precise geometric description of the scene. Taking as input a single image without other information, an automatic analysis technique fits a simple but perceptually consistent parametric 3D representation of the viewed space, which is used to drive an indirect constrained exploration method capable to provide the illusion of 3D exploration with realistic monocular (perspective and motion parallax) and binocular (stereo) depth cues. The effectiveness of the method is demonstrated on a variety of casual pictures and exploration configurations, including mobile devices.",
        "projects": [],
        "thumbnail_image": "2016-VMV-peep-thumb.jpg",
        "teaser_image": "2016-VMV-peep-teaser.jpg",
        "doi": "10.2312/vmv.20161347",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Agus:2016:PPE%27",
        "youtube_video": "S6EUIWyks6E",
        "bibtex_id": ""
    },
    {
        "code": "2016-STAG-crs4_vic",
        "title": "CRS4 Visual Computing",
        "authors": "Enrico Gobbetti, Marco Agus, Fabio Bettio, Alberto Jaspe-Villanueva, Fabio Marton, Ruggero Pintus, Giovanni Pintore, and Antonio Zorcolo",
        "year": "2016",
        "type" : "misc",
        "conference": "STAG 2016 Lab Presentations",
        "abstract": "This lab presentation briefly describes the Visual Computing group of the CRS4 research center. Established in 1996, the group primarily focuses on the study, development, and application of technology for acquisition, storage, processing, distribution, and interactive exploration of complex objects and environments. Research is widely published in major journals and conferences, and many of the developed technologies are used (or have been used) in as diverse real-world applications as internet geoviewing, scientific data analysis, surgical training, and cultural heritage study and valorization.",
        "projects": [""],
        "thumbnail_image": "2016-STAG-crs4_vic-thumb.jpg",
        "teaser_image": "2016-STAG-crs4_vic-teaser.jpg",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Gobbetti:2016:CVC%27",
        "bibtex_id": ""
    },
    {
        "code": "2016-I3D-ssvdags",
        "title": "SSVDAGs: Symmetry-aware Sparse Voxel DAGs",
        "authors": "Alberto Jaspe-Villanueva, Fabio Marton, and Enrico Gobbetti",
        "year": "2016",
        "type" : "conference",
        "conference": "ACM i3D",
        "awards": "Best paper selection",
        "abstract": "Voxelized representations of complex 3D scenes are widely used nowadays to accelerate visibility queries in many GPU rendering techniques. Since GPU memory is limited, it is important that these data structures can be kept within a strict memory budget. Recently, directed acyclic graphs (DAGs) have been successfully introduced to compress sparse voxel octrees (SVOs), but they are limited to sharing identical regions of space. In this paper, we show that a more efficient lossless compression of geometry can be achieved, while keeping the same visibility-query performance, by merging subtrees that are identical through a similarity transform, and by exploiting the skewed distribution of references to shared nodes to store child pointers using a variabile bit-rate encoding. We also describe how, by selecting plane reflections along the main grid directions as symmetry transforms, we can construct highly compressed GPU-friendly structures using a fully out-of-core method. Our results demonstrate that state-of-the-art compression and real-time tracing performance can be achieved on high-resolution voxelized representations of real-world scenes of very different characteristics, including large CAD models, 3D scans, and typical gaming models, leading, for instance, to real-time GPU in-core visualization with shading and shadows of the full Boeing 777 at sub-millimetric precision.",
        "projects": ["Massive models"],
        "thumbnail_image": "2016-I3D-ssvdags",
        "teaser_image": "2016-I3D-ssvdags",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Jaspe:2016:SSS%27",
        "youtube_video": "GmQswlkynP0",
        "bibtex_id": ""
    },
    {
        "code": "2016-book_VCEGDT-pbr_architecture_heritage",
        "title": "Virtual Reality and Point-based Rendering in Architecture and Heritage",
        "authors": "Omar A. Mures, Alberto Jaspe-Villanueva, Emilio J. Padrón, and Juan R. Rabuñal",
        "year": "2016",
        "type" : "book chapter",
        "abstract": "Virtual Reality has been a hot research topic since the appearance of computer graphics, but lately there have been huge advances in the form of high quality and affordable commodity hardware, for example with headsets such as the Oculus Rift. The Rift is an upcoming head-mounted virtual reality display, which will soon be available for the mainstream, along with other similar new VR hardware: Sulon Cortex, CastAR, Altergaze, etc. These new devices also offer new possibilities in the field of Augmented Reality, up to now limited to tablets and smartphones as far as commodity hardware is concerned. In fact, Virtual Reality and Augmented Reality technologies have now achieved the point where it can effectively be applied in in conjunction with the aforementioned workflow will yield great advantages for architects, engineers and heritage professionals alike. This article shows new possibilities of application for Virtual Reality and Augmented Reality with massive point clouds in real world architectural and heritage workflows.",
        "projects": ["Massive models", "Point clouds", "Cultral Heritage"],
        "thumbnail_image": "2016-book_VCEGDT-pbr_architecture_heritage",
        "teaser_image": "2016-book_VCEGDT-pbr_architecture_heritage",
        "doi": "10.4018/978-1-5225-0029-2",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mures:2016:VRP%27",
        "youtube_video": "3Zo8v1uk2yA",
        "bibtex_id": ""
    },
    {
        "code": "2016-book_EBDMOI-pcm",
        "title": "Point Cloud Manager: Applications of a Middleware for Managing Huge Point Clouds",
        "authors": "Omar A. Mures, Alberto Jaspe-Villanueva, Emilio J. Padrón, and Juan R. Rabuñal",
        "year": "2016",
        "type" : "book chapter",
        "book": "Effective Big Data Management and Opportunities for Implementation",
        "book-data": "Chapter 13, IGI Global, June 2016",
        "abstract": "Recent advances in acquisition technologies, such as LIDAR and photogrammetry, have brought back to popularity 3D point clouds in a lot of fields of application of Computer Graphics: Civil Engineering, Architecture, Topography, etc. These acquisition systems are producing an unprecedented amount of geometric data with additional attached information, resulting in huge datasets whose processing and storage requirements exceed usual approaches, presenting new challenges that can be addressed from a Big Data perspective by applying High Performance Computing and Computer Graphics techniques. This chapter presents a series of applications built on top of Point Cloud Manager (PCM), a middleware that provides an abstraction for point clouds with arbitrary attached data and makes it easy to perform out-of-core operations on them on commodity CPUs and GPUs. Hence, different kinds of real world applications are tackled, showing both real-time and offline examples, and render-oriented and computation-related operations as well.",
        "projects": ["Massive models", "Point clouds"],
        "thumbnail_image": "2016-book_EBDMOI-pcm-thumb.jpg",
        "teaser_image": "2016-book_EBDMOI-pcm-teaser.jpg",
        "doi": "10.4018/978-1-5225-0182-4.ch013",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mures:2016:PCM%27",
        "youtube_video": "3Zo8v1uk2yA",
        "bibtex_id": ""
    },
    {
        "code": "2015-JOCCH-montescan",
        "title": "Mont'e Scan: Effective Shape and Color Digitization of Cluttered 3D Artworks",
        "authors": "Fabio Bettio, Alberto Jaspe-Villanueva, Emilio Merella, Fabio Marton, Enrico Gobbetti, and Ruggero Pintus",
        "year": "2015",
        "type" : "journal",
        "journal": "ACM Journal on Computing and Cultural Heritage",
        "abstract": "We propose an approach for improving the digitization of shape and color of 3D artworks in a cluttered environment using 3D laser scanning and flash photography. In order to separate clutter from acquired material, semi-automated methods are employed to generate masks used to segment the range maps and the color photographs. This approach allows the removal of unwanted 3D and color data prior to the integration of acquired data in a 3D model. Sharp shadows generated by flash acquisition are easily handled by this masking process, and color deviations introduced by the flash light are corrected at the color blending step by taking into account the geometry of the object. The approach has been evaluated on a large scale acquisition campaign of the Mont'e Prama complex. This site contains an extraordinary collection of stone fragments from the Nuragic era, which depict small models of prehistoric nuraghe (cone-shaped stone towers), as well as larger-than-life archers, warriors, and boxers. The acquisition campaign has covered 37 statues mounted on metallic supports. Color and shape were acquired at a resolution of 0.25mm, which resulted in over 6200 range maps (about 1.3G valid samples) and 3817 photographs.",
        "projects": ["Massive models", "Point clouds", "Cultral Heritage"],
        "thumbnail_image": "2015-JOCCH-montescan-thumb.jpg",
        "teaser_image": "2015-JOCCH-montescan-teaser.jpg",
        "doi": "10.1145/2644823",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Bettio:2014:MES%27",
        "bibtex_id": ""
    },
    {
        "code": "2015-EG-chc+rt",
        "title": "CHC+RT: Coherent Hierarchical Culling for Ray Tracing",
        "authors": "Oliver Mattausch, Jiri Bittner, Alberto Jaspe-Villanueva, Enrico Gobbetti, Michael Wimmer, and Renato Pajarola",
        "year": "2015",
        "type" : "journal",
        "conference": "Eurographics 2015",
        "journal": "Computer Graphics Forum",
        "journal-data": "34(2): 537-548, 2015",
        "abstract": "We propose a new technique for in-core and out-of-core GPU ray tracing using a generalization of hierarchical occlusion culling in the style of the CHC++ method. Our method exploits the rasterization pipeline and hardware occlusion queries in order to create coherent batches of work for localized shader-based ray-tracing kernels. By combining hierarchies in both ray space and object space, the method is able to share intermediate traversal results among multiple rays. We exploit temporal coherence among similar ray sets between frames and also within the given frame. A suitable management of the current visibility state makes it possible to benefit from occlusion culling for less coherent ray types like diffuse reflections. Since large scenes are still a challenge for modern GPU ray tracers, our method is most useful for scenes with medium to high complexity, especially since our method inherently supports ray tracing highly complex scenes that do not fit in GPU memory. For in-core scenes our method is comparable to CUDA ray tracing and performs up to 5.94 times better than pure shader-based ray tracing.",
        "projects": ["Massive models"],
        "thumbnail_image": "2015-EG-chc+rt",
        "teaser_image": "2015-EG-chc+rt",
        "doi": "10.1111/cgf.12582",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mattausch:2015:CCH%27",
        "youtube_video": "4Ma920yhVHE",
        "bibtex_id": ""
    },
    {
        "code": "2014-VMV-soar",
        "title": "SOAR: Stochastic Optimization for Affine global point set Registration",
        "authors": "Marco Agus, Enrico Gobbetti, Alberto Jaspe-Villanueva, Claudio Mura, and Renato Pajarola",
        "year": "2014",
        "type" : "conference",
        "conference": "Workshop on Vision, Modeling and Visualization (VMV)",
        "abstract": "We introduce a stochastic algorithm for pairwise affine registration of partially overlapping 3D point clouds with unknown point correspondences. The algorithm recovers the globally optimal scale, rotation, and translation alignment parameters and is applicable in a variety of difficult settings, including very sparse, noisy, and outlier-ridden datasets that do not permit the computation of local descriptors. The technique is based on a stochastic approach for the global optimization of an alignment error function robust to noise and resistant to outliers. At each optimization step, it alternates between stochastically visiting a generalized BSP-tree representation of the current solution landscape to select a promising transformation, finding point-to-point correspondences using a GPU-accelerated technique, and incorporating new error values in the BSP tree. In contrast to previous work, instead of simply constructing the tree by guided random sampling, we exploit the problem structure through a low-cost local minimization process based on analytically solving absolute orientation problems using the current correspondences. We demonstrate the quality and performance of our method on a variety of large point sets with different scales, resolutions, and noise characteristics.",
        "projects": ["Point clouds"],
        "thumbnail_image": "2014-VMV-soar",
        "teaser_image": "2014-VMV-soar",
        "doi": "10.2312/vmv.20141282",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Agus:2014:SSO%27",
        "youtube_video": "FZJSfOIfwAI",
        "bibtex_id": ""
    },
    {
        "code": "2014-STAG-line_rasterization",
        "title": "Practical line rasterization for multi-resolution textures",
        "authors": "Javier Taibo, Alberto Jaspe Villanueva, Antonio Seoane, Marco Agus, and Luis Hernandez",
        "year": "2014",
        "type" : "conference",
        "conference": "Smart Tools and Apps for Graphics (STAG)",
        "abstract": "Draping 2D vectorial information over a 3D terrain elevation model is usually performed by real-time rendering to texture. In the case of linear feature representation, there are several specific problems using the texturing approach, specially when using multi-resolution textures. These problems are related to visual quality, aliasing artifacts and rendering performance. In this paper, we address the problems of 2D line rasterization on a multi-resolution texturing engine from a pragmatical point of view; some alternative solutions are presented, compared and evaluated. For each solution we have analyzed the visual quality, the impact on the rendering performance and the memory consumption. The study performed in this work is based on an OpenGL implementation of a clipmap-based multi-resolution texturing system, and is oriented towards the use of inexpensive consumer graphics hardware.",
        "projects": ["Terrain rendering"],
        "thumbnail_image": "2014-STAG-line_rasterization",
        "teaser_image": "2014-STAG-line_rasterization",
        "doi": "10.2312/stag.20141234",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Taibo:2014:PLR%27",
        "bibtex_id": ""
    },
    {
        "code": "2014-JOCCH-isocam",
        "title": "IsoCam: Interactive Visual Exploration of Massive Cultural Heritage Models on Large Projection Setups",
        "authors": "Fabio Marton, Marcos Balsa Rodriguez, Fabio Bettio, Marco Agus, Alberto Jaspe Villanueva, and Enrico Gobbetti",
        "year": "2014",
        "type" : "journal",
        "journal": "ACM Journal on Computing and Cultural Heritage",
        "abstract": "We introduce a novel user interface and system for exploring extremely detailed 3D models in a museum setting. 3D models and associated information are presented on a large projection surface controlled by a touch-enabled surface placed at a suitable distance in front of it. Our indirect user interface, dubbed IsoCam, combines an object-aware interactive camera controller with an interactive point-of-interest selector and is implemented within a scalable implementation based on multiresolution structures shared between the rendering and user interaction subsystems. The collision-free camera controller automatically supports the smooth transition from orbiting to proximal navigation, by exploiting a distance-field representation of the 3D object. The point-of-interest selector exploits a specialized view similarity computation to propose a few nearby easily reachable interesting 3D views from a large database, move the camera to the user-selected point of interest, and provide extra information through overlaid annotations of the target view. The capabilities of our approach have been demonstrated in a public event attended by thousands of people, which were offered the possibility to explore sub-millimetric reconstructions of 38 stone statues of the Mont'e Prama Nuragic complex, depicting larger-than-life human figures, and small models of prehistoric Nuraghe (cone-shaped stone towers). A follow-up of this work, using 2.5m-high projection screens, is now included in permanent exhibitions at two Archeological Museums. Results of a thorough user evaluation, involving quantitative and subjective measurements, are discussed.",
        "projects": ["Massive models"],
        "thumbnail_image": "2014-CAG-indoor_architectural_reconstruction",
        "teaser_image": "2014-CAG-indoor_architectural_reconstruction",
        "doi": "10.1145/2611519",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Marton:2014:IIV%27",
        "youtube_video": "X4pj8mDWDeU",
        "bibtex_id": ""
    },
    {
        "code": "2014-EG-poster_indoor_reconstruction",
        "title": "Reconstructing Complex Indoor Environments with Arbitrary Wall Orientations",
        "authors": "Claudio Mura, Alberto Jaspe Villanueva, Oliver Mattausch, Enrico Gobbetti, and Renato Pajarola",
        "year": "2014",
        "type" : "poster",
        "conference": "Eurographics",
        "abstract": "Reconstructing the architectural shape of indoor environments is a problem that is gaining increasing attention in the field of computer graphics. While some solutions have been proposed in recent years, cluttered environments with multiple rooms and non-vertical walls still represent a challenging input for state-of-the-art methods. We propose an occlusions-aware pipeline that extends current solutions to work with complex environments with arbitrary wall orientations.",
        "projects": ["Point clouds", "Indoor modelling"],
        "thumbnail_image": "2014-EG-poster_indoor_reconstruction",
        "teaser_image": "2014-EG-poster_indoor_reconstruction",
        "doi": "10.2312/egp.20141069",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mura:2014:RCI%27",
        "bibtex_id": ""
    },
    {
        "code": "2014-EG-exploremaps",
        "title": "ExploreMaps: Efficient Construction and Ubiquitous Exploration of Panoramic View Graphs of Complex 3D Environments",
        "authors": "Marco Di Benedetto, Fabio Ganovelli, Marcos Balsa Rodriguez, Alberto Jaspe Villanueva, Roberto Scopigno, and Enrico Gobbetti",
        "year": "2014",
        "type" : "journal",
        "conference": "Eurographics",
        "journal": "Computer Graphics Forum",
        "journal-data": "33(2): 459-468, 2014",
        "abstract": "We introduce a novel efficient technique for automatically transforming a generic renderable 3D scene into a simple graph representation named ExploreMaps, where nodes are nicely placed point of views, called probes, and arcs are smooth paths between neighboring probes. Each probe is associated with a panoramic image enriched with preferred viewing orientations, and each path with a panoramic video. Our GPU-accelerated unattended construction pipeline distributes probes so as to guarantee coverage of the scene while accounting for perceptual criteria before finding smooth, good looking paths between neighboring probes. Images and videos are precomputed at construction time with off-line photorealistic rendering engines, providing a convincing 3D visualization beyond the limits of current real-time graphics techniques. At run-time, the graph is exploited both for creating automatic scene indexes and movie previews of complex scenes and for supporting interactive exploration through a low-DOF assisted navigation interface and the visual indexing of the scene provided by the selected viewpoints. Due to negligible CPU overhead and very limited use of GPU functionality, real-time performance is achieved on emerging web-based environments based on WebGL even on low-powered mobile devices.",
        "projects": ["Massive models"],
        "thumbnail_image": "2014-EG-exploremaps",
        "teaser_image": "2014-EG-exploremaps",
        "doi": "10.1111/cgf.12334",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27DiBenedetto:2014:EEC%27",
        "youtube_video": "i6X8nykw4Vs",
        "bibtex_id": ""
    },
    {
        "code": "2014-CAG-indoor_architectural_reconstruction",
        "title": "Automatic Room Detection and Reconstruction in Cluttered Indoor Environments with Complex Room Layouts",
        "authors": "Claudio Mura, Oliver Mattausch, Alberto Jaspe Villanueva, Enrico Gobbetti, and Renato Pajarola",
        "year": "2014",
        "type" : "journal",
        "journal": "Computers & Graphics",
        "journal-data": "44: 20-32, November 2014",
        "abstract": "We present a robust approach for reconstructing the main architectural structure of complex indoor environments given a set of cluttered 3D input range scans. Our method uses an efficient occlusion-aware process to extract planar patches as candidate walls, separating them from clutter and coping with missing data, and automatically extracts the individual rooms that compose the environment by applying a diffusion process on the space partitioning induced by the candidate walls. This diffusion process, which has a natural interpretation in terms of heat propagation, makes our method robust to artifacts and other imperfections that occur in typical scanned data of interiors. For each room, our algorithm reconstructs an accurate polyhedral model by applying methods from robust statistics. We demonstrate the validity of our approach by evaluating it on both synthetic models and real-world 3D scans of indoor environments.",
        "projects": ["Point clouds", "Indoor modelling"],
        "thumbnail_image": "2014-JOCCH-isocam",
        "teaser_image": "2014-JOCCH-isocam",
        "doi": "10.1016/j.cag.2014.07.005",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mura:2014:ARD%27",
        "bibtex_id": ""
    },
    {
        "code": "2013-EVIS-lightfield_display_calibration",
        "title": "Automatic Geometric Calibration of Projector-based Light-field Displays",
        "authors": "Marco Agus, Enrico Gobbetti, Alberto Jaspe Villanueva, Giovanni Pintore, and Ruggero Pintus",
        "year": "2013",
        "type" : "conference",
        "conference": "EuroVis",
        "abstract": "Continuous multiview (light-field) projection-based displays employ an array of projectors, mirrors, and a selectively transmissive screen to produce a light field. By appropriately modeling the display geometry, the light beams can emulate the emission from physical objects at fixed spatial locations, providing multiple freely moving viewers the illusion of interacting with floating objects. This paper presents a novel calibration method for this class of displays using a single uncalibrated camera and four fiducial markers. Calibration starts from a simple parametric description of the display layout. First, individual projectors are calibrated through parametric optimization of an idealized pinhole model. Then, the overall display and projector parameterization is globally optimized. Finally, independently for each projector, remaining errors are corrected through a rational 2D warping function. The final parameters are available to rendering engines to quickly compute forward and backward projections. The technique is demonstrated in the calibration of a large-scale horizontal-parallax-only 35MPixels light field display.",
        "projects": [""],
        "thumbnail_image": "2013-EVIS-ligtfield_display_calibration",
        "teaser_image": "2013-EVIS-ligtfield_display_calibration",
        "doi": "",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Agus:2013:CDS%27",
        "bibtex_id": ""
    },
    {
        "code": "2013-CAD-robust_indoor_reconstruction",
        "title": "Robust Reconstruction of Interior Building Structures with Multiple Rooms under Clutter and Occlusions",
        "authors": "Claudio Mura, Oliver Mattausch, Alberto Jaspe Villanueva, Enrico Gobbetti, and Renato Pajarola",
        "year": "2013",
        "type" : "conference",
        "conference": "IEEE International Conference on Computer-Aided Design and Computer Graphics",
        "awards": "Selected for C&G Journal",
        "abstract": "We present a robust approach for reconstructing the architectural structure of complex indoor environments given a set of cluttered input scans. Our method first uses an efficient occlusion-aware process to extract planar patches as potential wall segments, separating them from clutter and coping with missing data. Using a diffusion process to further increase its robustness, our algorithm is able to reconstruct a clean architectural model from those potential wall segments. To our knowledge, this is the first indoor reconstruction method which goes beyond a binary classification and auto- matically recognizes different rooms as separate components. We demonstrate the validity of our approach by testing it on both synthetic models and real-world 3d scans of indoor environments.",
        "projects": [""],
        "thumbnail_image": "2013-CAD-robust_indoor_reconstruction",
        "teaser_image": "2013-CAD-robust_indoor_reconstruction",
        "doi": "10.1109/CADGraphics.2013.14",
        "lab_website": "http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id=%27Mura:2013:RRI%27",
        "bibtex_id": ""
    },
    {
        "code": "2012-VAR-inspeccion_catedral",
        "title": "Aplicación para la inspección espacial, volumtrica y seccional interactiva de la Catedral de Santiago de Compostela",
        "authors": "Viana Barneche, Luis Hernández, Alberto Jaspe-Villanueva, and Gustavo Fariña",
        "year": "2012",
        "type" : "journal",
        "journal": "Virtual Archeology Review",
        "journal-data": "3(6):78-82, 2012",
        "abstract": "This paper describes the design, production and implementation of an application for the formal analysis of the Cathedral of Santiago de Compostela. The geometrical complexity of the model of this building for the level of detail required, derived from the profusion of stylistic elements present, that constitutes one of its signs of identity leaded to use the progressive refinement radiosity method to generate a model which could be handled in real-time, adding the visual quality of globalillumination, to be implemented in an application that allows the user to interactively inspect and cross-section the model.",
        "projects": ["Architectectural Visualization"],
        "thumbnail_image": "2012-VAR-inspeccion_catedral",
        "teaser_image": "2012-VAR-inspeccion_catedral",
        "doi": "10.4995/var.2012.4448",
        "lab_website": "https://videalab.udc.es/CatedralSantiago",
        "youtube_video": "zO2vYpmJHiE",
        "bibtex_id": ""
    },
    {
        "code": "2011-EGA-space_perception",
        "title": "Space perception in Architectural Visualization through Immersive Virtual Reality",
        "authors": "Luis Hernández, Javier Taibo,. Antonio Soane, and Alberto Jaspe-Villanueva",
        "year": "2011",
        "type" : "journal",
        "conference": "",
        "journal": "Journal of Architectural Graphic Expression (EGA: Revista de expresión gráfica arquitectónica)",
        "abstract": "Immersive virtual reality constitutes a powerful tool for the vivid exploration of virtual spaces.The feeling of presence producedin the user by the action of seeing the digital space just pointing the view in any direction is greatly enhanced when adding the capability of walking physically during the experience. This paper describes aspects related with the experimentation of this hybrid space, real and virtual at the same time, that was implemented by the authors in an immersive virtual reality museum installation called 'The Empty Museum'.",
        "projects": ["Virtual Reality", "Architectectural Visualization"],
        "thumbnail_image": "2011-EGA-space_perception",
        "teaser_image": "2011-EGA-space_perception",
        "doi": "10.4995/ega.2011.1110",
        "lab_website": "https://polipapers.upv.es/index.php/EGA/article/view/1110/1179",
        "bibtex_id": ""
    },
    {
        "code": "2009-book_EAI-acceleration_ia_gpus",
        "title": "Acceleration of IA algorithms using GPUs",
        "authors": "Antonio Seoane and Alberto Jaspe-Villanueva",
        "year": "2009",
        "type" : "book chapter",
        "book": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2009-book_EAI-acceleration_ia_gpus",
        "teaser_image": "2009-book_EAI-acceleration_ia_gpus",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-SIGRADI-cuencas_visuales",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-SIGRADI-cuencas_visuales",
        "teaser_image": "2008-SIGRADI-cuencas_visuales",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-LaSalle-interfaces_naturales_museos",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-LaSalle-interfaces_naturales_museos",
        "teaser_image": "2008-LaSalle-interfaces_naturales_museos",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-ISICTCH-virtual_reality_museum",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-ISICTCH-virtual_reality_museum",
        "teaser_image": "2008-ISICTCH-virtual_reality_museum",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2008-book_GPU-Gems-mapping_large_textures",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2008-book_GPU-Gems-mapping_large_textures",
        "teaser_image": "2008-book_GPU-Gems-mapping_large_textures",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },
    {
        "code": "2007-WSCG-hardware_independet-clipmapping",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2007-WSCG-hardware_independet-clipmapping",
        "teaser_image": "2007-WSCG-hardware_independet-clipmapping",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2007-SIGPHI-museo_vacio_patrimonio",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2007-SIGPHI-museo_vacio_patrimonio",
        "teaser_image": "2007-SIGPHI-museo_vacio_patrimonio",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2007-IJAC-walking_digital_spaces",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2007-IJAC-walking_digital_spaces",
        "teaser_image": "2007-IJAC-walking_digital_spaces",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2006-SIGRADI-museo_vacio",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2006-SIGRADI-museo_vacio",
        "teaser_image": "2006-SIGRADI-museo_vacio",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2005-ICC-gid_integration",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2005-ICC-gid_integration",
        "teaser_image": "2005-ICC-gid_integration",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2005-cisit-gestion_trafico",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2005-cisit-gestion_trafico",
        "teaser_image": "2005-cisit-gestion_trafico",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    },        
    {
        "code": "2005-CAD-creativity_space",
        "title": "",
        "authors": "",
        "year": "",
        "type" : "",
        "conference": "",
        "journal": "",
        "awards": "",
        "abstract": "",
        "projects": [""],
        "thumbnail_image": "2005-CAD-creativity_space",
        "teaser_image": "2005-CAD-creativity_space",
        "doi": "",
        "lab_website": "",
        "youtube_video": "",
        "bibtex_id": ""
    }
]